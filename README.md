# BSc Appendix

Contains various scripts and benchmarks results of [my thesis](https://github.com/Pl8tinium/llm-discussion) for BSc.

## Benchmark results

| ID  | Tests                               | Benchmark                                                                                           | Results                                                                                                      | Description                                                                                                                                      | Amount of runs |
|-----|-------------------------------------|-----------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|----------------|
| DR#1| Direct reply vs unmodified reply    | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/DR%231/benchmark_results.csv](benchmark_results/DR%231/benchmark_results.csv)             | Measure impact of enforcing direct answers vs allowing reasoning/standard responses, to quantify whether reasoning improves correctness | 3              |
| DR#2| Direct reply vs unmodified reply    | [tasksource/proofwriter](https://huggingface.co/datasets/tasksource/proofwriter)                     | [benchmark_results/DR%232/benchmark_results.csv](benchmark_results/DR%232/benchmark_results.csv)             | Confirm the direct-vs-unmodified effect on a second benchmark to ensure the conclusion is not benchmark-specific                         | 3              |
| SA#1| Structural adherence                | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/SA%231/structured_output_adherence_results%20%231.csv](benchmark_results/SA%231/structured_output_adherence_results%20%231.csv) | Validate which models can reliably follow strict output structure schema, enabling a structural-adherence benchmark setup                | 2              |
| SA#2| Structural adherence (Reasoning)    | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/SA%232/structured_output_adherence_results.csv](benchmark_results/SA%232/structured_output_adherence_results.csv)     | Validate which reasoning-models can reliably follow strict output structure schema, enabling a structural-adherence benchmark setup      | 1              |
| RD#1| Reasoning disable                   | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/RD%231/reasoning_disable_results.csv](benchmark_results/RD%231/reasoning_disable_results.csv)                         | Determine which models support turning off reasoning (and which fail)                                                                    | 1              |
| SM#1| Small model + large model           | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/SM%231/benchmark_results.csv](benchmark_results/SM%231/benchmark_results.csv)             | Test whether a duo model approach yields intermediate/average performance, motivating cost–quality tradeoff analysis                     | 5              |
| SM#2| Small model + large model (FOSS)    | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/SM%232/benchmark_results.csv](benchmark_results/SM%232/benchmark_results.csv)             | Replicate the duo-model hypothesis using open/hostable models to make cost assumptions more transparent and comparable                   | 5              |
| SM#3| Small model + large model (FOSS)    | [tasksource/proofwriter](https://huggingface.co/datasets/tasksource/proofwriter)                     | [benchmark_results/SM%233/benchmark_results.csv](benchmark_results/SM%233/benchmark_results.csv)             | Validate whether the duo-model effect generalizes to a second benchmark; identify benchmark–method mismatch if it does not              | 3              |
| ST#1| Small vs large (reasoning)         | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/ST%231/benchmark_results.csv](benchmark_results/ST%231/benchmark_results.csv)             | Evaluate whether pairing reasoning models helps or hurts relative to the larger model and whether it produces intermediate-style behavior | 5              |
| ST#2| Large vs large (reasoning)          | [TAUR-Lab/MuSR](https://huggingface.co/datasets/TAUR-Lab/MuSR)                                      | [benchmark_results/ST%232/benchmark_results.csv](benchmark_results/ST%232/benchmark_results.csv)             | Test large-vs-large pairings to see whether positive synergy or other emergent patterns occur                                               | 5              |
| ST#3| Small vs large (reasoning)         | [tasksource/proofwriter](https://huggingface.co/datasets/tasksource/proofwriter)                     | [benchmark_results/ST%233/benchmark_results.csv](benchmark_results/ST%233/benchmark_results.csv)             | See whether small-vs-large duo behavior replicates on a second benchmark with different task requirements                      | 3              |

## Further tests

- [validate_extension_capabilities](/tests/validate_extension_capabilities.py) Initial script that validated the capabilities to extend an existing reasoning chain.